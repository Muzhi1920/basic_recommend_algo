{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_data = 'fm_data/xgb_train.txt'\n",
    "test_data = 'fm_data/xgb_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(y_hat, y_t, tip):\n",
    "    for i in range(len(y_hat)):\n",
    "        if(y_hat[i]>=0.5):\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            y_hat[i] = 0\n",
    "    acc = y_hat.ravel() == y_t.ravel()\n",
    "    print (acc)\n",
    "    print (tip + '正确率：\\t', float(acc.sum()) / y_hat.size)\n",
    "\n",
    "def preprocess(data):\n",
    "    feature=np.array(data.iloc[:,:-1])\n",
    "    label=data.iloc[:,-1]\n",
    "    label=np.array(label)\n",
    "    print(\"未归一化处理数据：\",feature[0],', length: ',len(feature[0]),label[0])\n",
    "    return feature,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "未归一化处理数据： [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] , length:  640 0.0\n未归一化处理数据： [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] , length:  640 0.0\n"
    }
   ],
   "source": [
    "train = pd.read_csv(train_data,header=None)\n",
    "test = pd.read_csv(test_data,header=None)\n",
    "x_train, y_train = preprocess(train)\n",
    "x_test, y_test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.92034494 0.07965506]\n [0.49068357 0.50931643]\n [0.72643193 0.27356807]\n [0.88687478 0.11312522]\n [0.83333271 0.16666729]\n [0.18913647 0.81086353]\n [0.85315564 0.14684436]\n [0.75232043 0.24767957]\n [0.83163706 0.16836294]\n [0.8890252  0.1109748 ]\n [0.86838475 0.13161525]\n [0.886906   0.113094  ]\n [0.95170476 0.04829524]\n [0.92440783 0.07559217]\n [0.50918562 0.49081438]\n [0.37409316 0.62590684]\n [0.6185447  0.3814553 ]\n [0.79664759 0.20335241]\n [0.67420631 0.32579369]\n [0.9660057  0.0339943 ]\n [0.64391955 0.35608045]\n [0.87539659 0.12460341]\n [0.33894003 0.66105997]\n [0.55206073 0.44793927]\n [0.95438055 0.04561945]\n [0.9660057  0.0339943 ]\n [0.88026203 0.11973797]\n [0.83717478 0.16282522]\n [0.90051915 0.09948085]\n [0.75960143 0.24039857]\n [0.5148238  0.4851762 ]\n [0.62034821 0.37965179]\n [0.59426357 0.40573643]\n [0.79064391 0.20935609]\n [0.2290194  0.7709806 ]\n [0.38235091 0.61764909]\n [0.8412227  0.1587773 ]\n [0.60366532 0.39633468]\n [0.2871072  0.7128928 ]\n [0.40328239 0.59671761]\n [0.68248662 0.31751338]\n [0.62371057 0.37628943]\n [0.75652847 0.24347153]\n [0.92721874 0.07278126]\n [0.10556861 0.89443139]\n [0.12217192 0.87782808]\n [0.6564069  0.3435931 ]\n [0.08790469 0.91209531]\n [0.3172747  0.6827253 ]\n [0.838045   0.161955  ]\n [0.91682802 0.08317198]\n [0.38269536 0.61730464]\n [0.95672565 0.04327435]\n [0.88182171 0.11817829]\n [0.69229221 0.30770779]\n [0.69949485 0.30050515]\n [0.33999777 0.66000223]\n [0.50785212 0.49214788]\n [0.78349136 0.21650864]\n [0.59624246 0.40375754]\n [0.25505073 0.74494927]\n [0.86620382 0.13379618]\n [0.78930456 0.21069544]\n [0.90388911 0.09611089]\n [0.89004907 0.10995093]\n [0.82516887 0.17483113]\n [0.75361018 0.24638982]\n [0.35595996 0.64404004]\n [0.58756083 0.41243917]\n [0.82499532 0.17500468]\n [0.78575245 0.21424755]\n [0.86320088 0.13679912]\n [0.85062693 0.14937307]\n [0.6482291  0.3517709 ]\n [0.74139689 0.25860311]\n [0.8645869  0.1354131 ]\n [0.65348992 0.34651008]\n [0.43286421 0.56713579]\n [0.43934363 0.56065637]\n [0.6702447  0.3297553 ]\n [0.87539659 0.12460341]\n [0.52647344 0.47352656]\n [0.63929097 0.36070903]\n [0.45447744 0.54552256]\n [0.9660057  0.0339943 ]\n [0.51991603 0.48008397]\n [0.86392322 0.13607678]\n [0.08584637 0.91415363]\n [0.9531417  0.0468583 ]\n [0.29354304 0.70645696]\n [0.69111014 0.30888986]\n [0.46935851 0.53064149]\n [0.78353053 0.21646947]\n [0.44415652 0.55584348]\n [0.16456813 0.83543187]\n [0.69140608 0.30859392]\n [0.90738147 0.09261853]\n [0.26866214 0.73133786]\n [0.94642313 0.05357687]\n [0.92912177 0.07087823]\n [0.9162861  0.0837139 ]\n [0.69649665 0.30350335]\n [0.29962778 0.70037222]\n [0.53251738 0.46748262]\n [0.59843424 0.40156576]\n [0.33288335 0.66711665]\n [0.9660057  0.0339943 ]\n [0.68255558 0.31744442]\n [0.94049117 0.05950883]\n [0.76255277 0.23744723]\n [0.09849474 0.90150526]\n [0.18011139 0.81988861]\n [0.49938113 0.50061887]\n [0.32751176 0.67248824]\n [0.87761609 0.12238391]\n [0.40668416 0.59331584]\n [0.9660057  0.0339943 ]\n [0.33948612 0.66051388]\n [0.76114202 0.23885798]\n [0.81359576 0.18640424]\n [0.84686195 0.15313805]\n [0.15843533 0.84156467]\n [0.82600994 0.17399006]\n [0.79142987 0.20857013]\n [0.74326572 0.25673428]\n [0.82468838 0.17531162]\n [0.50970935 0.49029065]\n [0.51999238 0.48000762]\n [0.92440783 0.07559217]\n [0.53073532 0.46926468]\n [0.70125233 0.29874767]\n [0.90212016 0.09787984]\n [0.81848785 0.18151215]\n [0.92848382 0.07151618]\n [0.41351148 0.58648852]\n [0.49725643 0.50274357]\n [0.92776444 0.07223556]\n [0.52757488 0.47242512]\n [0.9171647  0.0828353 ]\n [0.71841253 0.28158747]\n [0.54545115 0.45454885]\n [0.74044266 0.25955734]\n [0.57845653 0.42154347]\n [0.75101816 0.24898184]\n [0.29812237 0.70187763]\n [0.36402701 0.63597299]\n [0.12953061 0.87046939]\n [0.65971163 0.34028837]\n [0.8764206  0.1235794 ]\n [0.94170843 0.05829157]\n [0.83216243 0.16783757]\n [0.7771373  0.2228627 ]\n [0.86649877 0.13350123]\n [0.80715297 0.19284703]\n [0.27747093 0.72252907]\n [0.94800075 0.05199925]\n [0.28644966 0.71355034]\n [0.46458093 0.53541907]\n [0.76649627 0.23350373]\n [0.48667544 0.51332456]\n [0.18245639 0.81754361]\n [0.1644217  0.8355783 ]\n [0.32675871 0.67324129]\n [0.50357932 0.49642068]\n [0.52854727 0.47145273]\n [0.63398832 0.36601168]\n [0.4763159  0.5236841 ]\n [0.51367224 0.48632776]\n [0.41712279 0.58287721]\n [0.08833184 0.91166816]\n [0.9660057  0.0339943 ]\n [0.89899367 0.10100633]\n [0.4004915  0.5995085 ]\n [0.86307238 0.13692762]\n [0.14432315 0.85567685]\n [0.46010537 0.53989463]\n [0.90826012 0.09173988]\n [0.82500619 0.17499381]\n [0.93238893 0.06761107]\n [0.9660057  0.0339943 ]\n [0.29173602 0.70826398]\n [0.82104385 0.17895615]\n [0.62732646 0.37267354]\n [0.78962278 0.21037722]\n [0.68248662 0.31751338]\n [0.63873587 0.36126413]\n [0.63407012 0.36592988]\n [0.86257499 0.13742501]\n [0.39739791 0.60260209]\n [0.86579851 0.13420149]\n [0.18013558 0.81986442]\n [0.57159692 0.42840308]\n [0.50199249 0.49800751]\n [0.9660057  0.0339943 ]\n [0.35556514 0.64443486]\n [0.43318645 0.56681355]\n [0.9531417  0.0468583 ]\n [0.72292821 0.27707179]\n [0.55379249 0.44620751]\n [0.84469299 0.15530701]\n [0.51989666 0.48010334]\n [0.12685893 0.87314107]\n [0.32957984 0.67042016]\n [0.7974236  0.2025764 ]\n [0.67640066 0.32359934]\n [0.82075033 0.17924967]\n [0.49882662 0.50117338]\n [0.21762683 0.78237317]\n [0.82654108 0.17345892]\n [0.51132178 0.48867822]\n [0.57212371 0.42787629]\n [0.53618549 0.46381451]\n [0.69587536 0.30412464]\n [0.49020042 0.50979958]\n [0.1460278  0.8539722 ]\n [0.08959473 0.91040527]\n [0.88149546 0.11850454]\n [0.72599193 0.27400807]\n [0.91823293 0.08176707]\n [0.78349136 0.21650864]\n [0.84479872 0.15520128]\n [0.62228158 0.37771842]\n [0.65237264 0.34762736]\n [0.39288743 0.60711257]\n [0.61532526 0.38467474]\n [0.84918833 0.15081167]\n [0.65471442 0.34528558]\n [0.27160652 0.72839348]\n [0.8906319  0.1093681 ]\n [0.58687163 0.41312837]\n [0.73714638 0.26285362]\n [0.1088117  0.8911883 ]\n [0.86138738 0.13861262]\n [0.82178628 0.17821372]\n [0.89169872 0.10830128]\n [0.85067685 0.14932315]\n [0.75981383 0.24018617]\n [0.81925376 0.18074624]\n [0.63609003 0.36390997]\n [0.34277268 0.65722732]\n [0.74761347 0.25238653]\n [0.838045   0.161955  ]\n [0.29240381 0.70759619]\n [0.33431885 0.66568115]\n [0.76457747 0.23542253]\n [0.39048916 0.60951084]\n [0.61695079 0.38304921]\n [0.12208402 0.87791598]\n [0.60572014 0.39427986]\n [0.51468361 0.48531639]\n [0.87428426 0.12571574]\n [0.84912712 0.15087288]\n [0.44000905 0.55999095]\n [0.37761995 0.62238005]\n [0.25898669 0.74101331]\n [0.59972099 0.40027901]\n [0.67771904 0.32228096]\n [0.76762533 0.23237467]\n [0.5546885  0.4453115 ]\n [0.89844899 0.10155101]\n [0.10081673 0.89918327]\n [0.8835722  0.1164278 ]\n [0.54038468 0.45961532]\n [0.83943798 0.16056202]\n [0.83140272 0.16859728]\n [0.29115012 0.70884988]\n [0.95672565 0.04327435]]\nNormalized Cross Entropy 0.8767969031281931\n"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2',C=0.05)\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred_test = lr.predict_proba(x_test)\n",
    "\n",
    "print(y_pred_test)\n",
    "NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))\n",
    "print(\"Normalized Cross Entropy \" + str(NE))\n",
    "\n",
    "y_hat_test=np.array([0 if p[0]>=0.5 else 1  for p in y_pred_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ True  True  True  True  True  True  True  True  True False  True  True\n  True  True False  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True False\n  True  True  True  True False False  True  True  True  True  True False\n False  True  True False  True  True  True  True False  True  True False\n  True  True  True  True  True  True  True False False  True  True  True\n  True  True  True  True False False  True False  True  True  True  True\n  True False  True  True  True  True  True  True  True False  True  True\n  True  True  True  True  True  True  True False  True  True  True  True\n  True  True  True  True False  True  True False  True  True False  True\n  True False  True  True  True  True  True  True  True False  True  True\n  True  True  True False  True False  True  True  True False  True  True\n False  True  True False  True  True  True  True  True  True  True  True\n False False False False  True  True  True False  True False  True  True\n False False  True  True False  True  True  True  True False  True  True\n  True  True False  True  True  True  True  True  True  True  True  True\n False  True  True  True  True  True  True  True False  True False  True\n  True False False  True False  True  True False  True False  True  True\n  True  True False  True  True False  True False  True  True  True False\n  True False False  True  True  True  True  True  True  True False  True\n  True  True  True False  True  True  True  True False False  True  True\n  True  True  True  True False  True False  True  True  True  True  True\n  True  True  True]\nXGBoost+LR正确率：\t 0.7790262172284644\n"
    }
   ],
   "source": [
    "show_accuracy(y_hat_test,y_test,'XGBoost+LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}